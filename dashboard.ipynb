{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import single_node\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'single_node' from '/home/john/projects/federated-learning-example-for-student/single_node.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(single_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda with {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
      "Load User Data:  type-one-9-150000-samples.csv\n",
      "Load User Data:  type-four-8-150000-samples.csv\n",
      "Load User Data:  type-one-7-150000-samples.csv\n",
      "Load User Data:  type-total-8-150000-samples.csv\n",
      "Load User Data:  type-one-1-150000-samples.csv\n",
      "Load User Data:  type-two-6-150000-samples.csv\n",
      "Load User Data:  type-one-8-150000-samples.csv\n",
      "Load User Data:  type-two-4-150000-samples.csv\n",
      "Load User Data:  type-one-2-150000-samples.csv\n",
      "Load User Data:  type-four-4-150000-samples.csv\n",
      "Load User Data:  type-two-2-150000-samples.csv\n",
      "Load User Data:  type-four-0-150000-samples.csv\n",
      "Load User Data:  type-one-3-150000-samples.csv\n",
      "Load User Data:  type-two-0-150000-samples.csv\n",
      "Load User Data:  type-one-4-150000-samples.csv\n",
      "Load User Data:  type-one-0-150000-samples.csv\n",
      "Load User Data:  type-one-10-150000-samples.csv\n",
      "Load User Data:  type-one-6-150000-samples.csv\n",
      "Load User Data:  type-two-10-150000-samples.csv\n",
      "Load User Data:  type-one-5-150000-samples.csv\n",
      "Load User Data:  type-two-8-150000-samples.csv\n",
      "Train Epoch: 1 [1024/2574213 (0%)]\tLoss: 2.685979\n",
      "Train Epoch: 1 [258048/2574213 (10%)]\tLoss: 2.450927\n",
      "Train Epoch: 1 [515072/2574213 (20%)]\tLoss: 2.278569\n",
      "Train Epoch: 1 [772096/2574213 (30%)]\tLoss: 2.128664\n",
      "Train Epoch: 1 [1029120/2574213 (40%)]\tLoss: 2.100051\n",
      "Train Epoch: 1 [1286144/2574213 (50%)]\tLoss: 2.002013\n",
      "Train Epoch: 1 [1543168/2574213 (60%)]\tLoss: 1.821920\n",
      "Train Epoch: 1 [1800192/2574213 (70%)]\tLoss: 1.791161\n",
      "Train Epoch: 1 [2057216/2574213 (80%)]\tLoss: 1.806942\n",
      "Train Epoch: 1 [2314240/2574213 (90%)]\tLoss: 1.684932\n",
      "Train Epoch: 1 [2571264/2574213 (100%)]\tLoss: 1.635108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.40      0.51     94233\n",
      "           2       0.72      0.83      0.77     94411\n",
      "           3       0.50      0.37      0.43     93535\n",
      "           4       0.57      0.40      0.47     94022\n",
      "           6       0.64      0.52      0.58     93815\n",
      "           7       0.58      0.87      0.70     94011\n",
      "           8       0.49      0.49      0.49     94188\n",
      "           9       0.89      0.56      0.69     93903\n",
      "          10       0.51      0.73      0.60     94359\n",
      "          11       0.60      0.84      0.70     94252\n",
      "          12       0.50      0.55      0.53     94254\n",
      "          13       0.96      0.94      0.95     68252\n",
      "\n",
      "    accuracy                           0.62   1103235\n",
      "   macro avg       0.64      0.63      0.62   1103235\n",
      "weighted avg       0.63      0.62      0.61   1103235\n",
      "\n",
      "Train Epoch: 2 [1024/2574213 (0%)]\tLoss: 1.681153\n",
      "Train Epoch: 2 [258048/2574213 (10%)]\tLoss: 1.596367\n",
      "Train Epoch: 2 [515072/2574213 (20%)]\tLoss: 1.637803\n",
      "Train Epoch: 2 [772096/2574213 (30%)]\tLoss: 1.465603\n",
      "Train Epoch: 2 [1029120/2574213 (40%)]\tLoss: 1.501618\n",
      "Train Epoch: 2 [1286144/2574213 (50%)]\tLoss: 1.373805\n",
      "Train Epoch: 2 [1543168/2574213 (60%)]\tLoss: 1.355497\n",
      "Train Epoch: 2 [1800192/2574213 (70%)]\tLoss: 1.291083\n",
      "Train Epoch: 2 [2057216/2574213 (80%)]\tLoss: 1.288041\n",
      "Train Epoch: 2 [2314240/2574213 (90%)]\tLoss: 1.306019\n",
      "Train Epoch: 2 [2571264/2574213 (100%)]\tLoss: 1.238826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.48      0.53     94233\n",
      "           2       0.91      0.98      0.94     94411\n",
      "           3       0.69      0.76      0.73     93535\n",
      "           4       0.67      0.51      0.58     94022\n",
      "           6       0.67      0.55      0.61     93815\n",
      "           7       0.87      0.97      0.91     94011\n",
      "           8       0.82      0.71      0.76     94188\n",
      "           9       0.96      0.77      0.86     93903\n",
      "          10       0.71      0.74      0.73     94359\n",
      "          11       0.69      0.87      0.77     94252\n",
      "          12       0.49      0.66      0.56     94254\n",
      "          13       0.98      0.96      0.97     68252\n",
      "\n",
      "    accuracy                           0.74   1103235\n",
      "   macro avg       0.75      0.75      0.74   1103235\n",
      "weighted avg       0.75      0.74      0.74   1103235\n",
      "\n",
      "Train Epoch: 3 [1024/2574213 (0%)]\tLoss: 1.230815\n",
      "Train Epoch: 3 [258048/2574213 (10%)]\tLoss: 1.210470\n",
      "Train Epoch: 3 [515072/2574213 (20%)]\tLoss: 1.121902\n",
      "Train Epoch: 3 [772096/2574213 (30%)]\tLoss: 1.113150\n",
      "Train Epoch: 3 [1029120/2574213 (40%)]\tLoss: 1.066015\n",
      "Train Epoch: 3 [1286144/2574213 (50%)]\tLoss: 1.094274\n",
      "Train Epoch: 3 [1543168/2574213 (60%)]\tLoss: 1.032752\n",
      "Train Epoch: 3 [1800192/2574213 (70%)]\tLoss: 1.061654\n",
      "Train Epoch: 3 [2057216/2574213 (80%)]\tLoss: 1.030677\n",
      "Train Epoch: 3 [2314240/2574213 (90%)]\tLoss: 0.938406\n",
      "Train Epoch: 3 [2571264/2574213 (100%)]\tLoss: 0.886075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.66      0.67     94233\n",
      "           2       0.91      0.99      0.94     94411\n",
      "           3       0.76      0.85      0.80     93535\n",
      "           4       0.80      0.51      0.62     94022\n",
      "           6       0.70      0.60      0.65     93815\n",
      "           7       0.90      0.98      0.94     94011\n",
      "           8       0.87      0.80      0.83     94188\n",
      "           9       0.97      0.79      0.87     93903\n",
      "          10       0.77      0.74      0.75     94359\n",
      "          11       0.70      0.94      0.80     94252\n",
      "          12       0.59      0.73      0.65     94254\n",
      "          13       0.98      0.96      0.97     68252\n",
      "\n",
      "    accuracy                           0.79   1103235\n",
      "   macro avg       0.80      0.80      0.79   1103235\n",
      "weighted avg       0.80      0.79      0.79   1103235\n",
      "\n",
      "Train Epoch: 4 [1024/2574213 (0%)]\tLoss: 0.923778\n",
      "Train Epoch: 4 [258048/2574213 (10%)]\tLoss: 0.856143\n",
      "Train Epoch: 4 [515072/2574213 (20%)]\tLoss: 0.948712\n",
      "Train Epoch: 4 [772096/2574213 (30%)]\tLoss: 0.837079\n",
      "Train Epoch: 4 [1029120/2574213 (40%)]\tLoss: 0.785897\n",
      "Train Epoch: 4 [1286144/2574213 (50%)]\tLoss: 0.762492\n",
      "Train Epoch: 4 [1543168/2574213 (60%)]\tLoss: 0.799471\n",
      "Train Epoch: 4 [1800192/2574213 (70%)]\tLoss: 0.830863\n",
      "Train Epoch: 4 [2057216/2574213 (80%)]\tLoss: 0.788331\n",
      "Train Epoch: 4 [2314240/2574213 (90%)]\tLoss: 0.739650\n",
      "Train Epoch: 4 [2571264/2574213 (100%)]\tLoss: 0.766380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75     94233\n",
      "           2       0.96      0.99      0.97     94411\n",
      "           3       0.86      0.88      0.87     93535\n",
      "           4       0.87      0.63      0.73     94022\n",
      "           6       0.75      0.67      0.71     93815\n",
      "           7       0.90      0.98      0.94     94011\n",
      "           8       0.88      0.84      0.86     94188\n",
      "           9       0.98      0.91      0.94     93903\n",
      "          10       0.84      0.80      0.82     94359\n",
      "          11       0.73      0.94      0.82     94252\n",
      "          12       0.70      0.73      0.71     94254\n",
      "          13       0.99      0.97      0.98     68252\n",
      "\n",
      "    accuracy                           0.84   1103235\n",
      "   macro avg       0.85      0.84      0.84   1103235\n",
      "weighted avg       0.84      0.84      0.84   1103235\n",
      "\n",
      "Train Epoch: 5 [1024/2574213 (0%)]\tLoss: 0.803415\n",
      "Train Epoch: 5 [258048/2574213 (10%)]\tLoss: 0.719258\n",
      "Train Epoch: 5 [515072/2574213 (20%)]\tLoss: 0.715333\n",
      "Train Epoch: 5 [772096/2574213 (30%)]\tLoss: 0.742663\n",
      "Train Epoch: 5 [1029120/2574213 (40%)]\tLoss: 0.651517\n",
      "Train Epoch: 5 [1286144/2574213 (50%)]\tLoss: 0.756434\n",
      "Train Epoch: 5 [1543168/2574213 (60%)]\tLoss: 0.636826\n",
      "Train Epoch: 5 [1800192/2574213 (70%)]\tLoss: 0.614263\n",
      "Train Epoch: 5 [2057216/2574213 (80%)]\tLoss: 0.664998\n",
      "Train Epoch: 5 [2314240/2574213 (90%)]\tLoss: 0.727981\n",
      "Train Epoch: 5 [2571264/2574213 (100%)]\tLoss: 0.751312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79     94233\n",
      "           2       0.96      0.99      0.97     94411\n",
      "           3       0.91      0.94      0.92     93535\n",
      "           4       0.90      0.74      0.81     94022\n",
      "           6       0.76      0.71      0.73     93815\n",
      "           7       0.90      0.98      0.94     94011\n",
      "           8       0.90      0.86      0.88     94188\n",
      "           9       0.98      0.94      0.96     93903\n",
      "          10       0.85      0.81      0.83     94359\n",
      "          11       0.74      0.94      0.83     94252\n",
      "          12       0.81      0.73      0.77     94254\n",
      "          13       0.99      0.98      0.98     68252\n",
      "\n",
      "    accuracy                           0.87   1103235\n",
      "   macro avg       0.87      0.87      0.87   1103235\n",
      "weighted avg       0.87      0.87      0.87   1103235\n",
      "\n",
      "Train Epoch: 6 [1024/2574213 (0%)]\tLoss: 0.603194\n",
      "Train Epoch: 6 [258048/2574213 (10%)]\tLoss: 0.604650\n",
      "Train Epoch: 6 [515072/2574213 (20%)]\tLoss: 0.652927\n",
      "Train Epoch: 6 [772096/2574213 (30%)]\tLoss: 0.684065\n",
      "Train Epoch: 6 [1029120/2574213 (40%)]\tLoss: 0.585466\n",
      "Train Epoch: 6 [1286144/2574213 (50%)]\tLoss: 0.611722\n",
      "Train Epoch: 6 [1543168/2574213 (60%)]\tLoss: 0.558472\n",
      "Train Epoch: 6 [1800192/2574213 (70%)]\tLoss: 0.569311\n",
      "Train Epoch: 6 [2057216/2574213 (80%)]\tLoss: 0.551576\n",
      "Train Epoch: 6 [2314240/2574213 (90%)]\tLoss: 0.564161\n",
      "Train Epoch: 6 [2571264/2574213 (100%)]\tLoss: 0.547076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85     94233\n",
      "           2       0.96      0.99      0.98     94411\n",
      "           3       0.92      0.94      0.93     93535\n",
      "           4       0.98      0.76      0.85     94022\n",
      "           6       0.77      0.72      0.75     93815\n",
      "           7       0.91      0.98      0.94     94011\n",
      "           8       0.90      0.87      0.88     94188\n",
      "           9       0.98      0.94      0.96     93903\n",
      "          10       0.90      0.91      0.91     94359\n",
      "          11       0.77      0.95      0.85     94252\n",
      "          12       0.83      0.80      0.81     94254\n",
      "          13       0.99      0.98      0.98     68252\n",
      "\n",
      "    accuracy                           0.89   1103235\n",
      "   macro avg       0.90      0.89      0.89   1103235\n",
      "weighted avg       0.89      0.89      0.89   1103235\n",
      "\n",
      "Train Epoch: 7 [1024/2574213 (0%)]\tLoss: 0.589689\n",
      "Train Epoch: 7 [258048/2574213 (10%)]\tLoss: 0.516696\n",
      "Train Epoch: 7 [515072/2574213 (20%)]\tLoss: 0.600391\n",
      "Train Epoch: 7 [772096/2574213 (30%)]\tLoss: 0.579685\n",
      "Train Epoch: 7 [1029120/2574213 (40%)]\tLoss: 0.547166\n",
      "Train Epoch: 7 [1286144/2574213 (50%)]\tLoss: 0.493179\n",
      "Train Epoch: 7 [1543168/2574213 (60%)]\tLoss: 0.498982\n",
      "Train Epoch: 7 [1800192/2574213 (70%)]\tLoss: 0.523345\n",
      "Train Epoch: 7 [2057216/2574213 (80%)]\tLoss: 0.494082\n",
      "Train Epoch: 7 [2314240/2574213 (90%)]\tLoss: 0.519963\n",
      "Train Epoch: 7 [2571264/2574213 (100%)]\tLoss: 0.544988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88     94233\n",
      "           2       0.96      0.99      0.98     94411\n",
      "           3       0.92      0.94      0.93     93535\n",
      "           4       0.98      0.79      0.87     94022\n",
      "           6       0.78      0.74      0.76     93815\n",
      "           7       0.91      0.99      0.94     94011\n",
      "           8       0.91      0.88      0.90     94188\n",
      "           9       0.98      0.94      0.96     93903\n",
      "          10       0.92      0.91      0.92     94359\n",
      "          11       0.79      0.95      0.86     94252\n",
      "          12       0.86      0.81      0.83     94254\n",
      "          13       0.99      0.98      0.98     68252\n",
      "\n",
      "    accuracy                           0.90   1103235\n",
      "   macro avg       0.90      0.90      0.90   1103235\n",
      "weighted avg       0.90      0.90      0.90   1103235\n",
      "\n",
      "Train Epoch: 8 [1024/2574213 (0%)]\tLoss: 0.538486\n",
      "Train Epoch: 8 [258048/2574213 (10%)]\tLoss: 0.426484\n",
      "Train Epoch: 8 [515072/2574213 (20%)]\tLoss: 0.536207\n",
      "Train Epoch: 8 [772096/2574213 (30%)]\tLoss: 0.484272\n",
      "Train Epoch: 8 [1029120/2574213 (40%)]\tLoss: 0.477883\n",
      "Train Epoch: 8 [1286144/2574213 (50%)]\tLoss: 0.561054\n",
      "Train Epoch: 8 [1543168/2574213 (60%)]\tLoss: 0.419422\n",
      "Train Epoch: 8 [1800192/2574213 (70%)]\tLoss: 0.476655\n",
      "Train Epoch: 8 [2057216/2574213 (80%)]\tLoss: 0.450799\n",
      "Train Epoch: 8 [2314240/2574213 (90%)]\tLoss: 0.454281\n",
      "Train Epoch: 8 [2571264/2574213 (100%)]\tLoss: 0.409374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89     94233\n",
      "           2       0.97      0.99      0.98     94411\n",
      "           3       0.92      0.94      0.93     93535\n",
      "           4       0.98      0.82      0.89     94022\n",
      "           6       0.80      0.75      0.77     93815\n",
      "           7       0.91      0.99      0.94     94011\n",
      "           8       0.91      0.89      0.90     94188\n",
      "           9       0.98      0.94      0.96     93903\n",
      "          10       0.92      0.91      0.92     94359\n",
      "          11       0.81      0.95      0.88     94252\n",
      "          12       0.86      0.82      0.84     94254\n",
      "          13       0.99      0.98      0.98     68252\n",
      "\n",
      "    accuracy                           0.91   1103235\n",
      "   macro avg       0.91      0.91      0.91   1103235\n",
      "weighted avg       0.91      0.91      0.91   1103235\n",
      "\n",
      "Train Epoch: 9 [1024/2574213 (0%)]\tLoss: 0.493456\n",
      "Train Epoch: 9 [258048/2574213 (10%)]\tLoss: 0.431603\n",
      "Train Epoch: 9 [515072/2574213 (20%)]\tLoss: 0.434181\n",
      "Train Epoch: 9 [772096/2574213 (30%)]\tLoss: 0.448152\n",
      "Train Epoch: 9 [1029120/2574213 (40%)]\tLoss: 0.434036\n",
      "Train Epoch: 9 [1286144/2574213 (50%)]\tLoss: 0.461559\n",
      "Train Epoch: 9 [1543168/2574213 (60%)]\tLoss: 0.432951\n",
      "Train Epoch: 9 [1800192/2574213 (70%)]\tLoss: 0.386871\n",
      "Train Epoch: 9 [2057216/2574213 (80%)]\tLoss: 0.402488\n",
      "Train Epoch: 9 [2314240/2574213 (90%)]\tLoss: 0.372015\n",
      "Train Epoch: 9 [2571264/2574213 (100%)]\tLoss: 0.373322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     94233\n",
      "           2       0.97      0.99      0.98     94411\n",
      "           3       0.92      0.94      0.93     93535\n",
      "           4       0.98      0.85      0.91     94022\n",
      "           6       0.82      0.75      0.79     93815\n",
      "           7       0.91      0.99      0.94     94011\n",
      "           8       0.91      0.90      0.90     94188\n",
      "           9       0.98      0.94      0.96     93903\n",
      "          10       0.92      0.91      0.92     94359\n",
      "          11       0.84      0.95      0.89     94252\n",
      "          12       0.85      0.82      0.84     94254\n",
      "          13       0.99      0.98      0.98     68252\n",
      "\n",
      "    accuracy                           0.91   1103235\n",
      "   macro avg       0.91      0.91      0.91   1103235\n",
      "weighted avg       0.91      0.91      0.91   1103235\n",
      "\n",
      "Train Epoch: 10 [1024/2574213 (0%)]\tLoss: 0.365105\n",
      "Train Epoch: 10 [258048/2574213 (10%)]\tLoss: 0.340917\n",
      "Train Epoch: 10 [515072/2574213 (20%)]\tLoss: 0.386127\n",
      "Train Epoch: 10 [772096/2574213 (30%)]\tLoss: 0.377600\n",
      "Train Epoch: 10 [1029120/2574213 (40%)]\tLoss: 0.438172\n",
      "Train Epoch: 10 [1286144/2574213 (50%)]\tLoss: 0.312491\n",
      "Train Epoch: 10 [1543168/2574213 (60%)]\tLoss: 0.398534\n",
      "Train Epoch: 10 [1800192/2574213 (70%)]\tLoss: 0.395632\n",
      "Train Epoch: 10 [2057216/2574213 (80%)]\tLoss: 0.405823\n",
      "Train Epoch: 10 [2314240/2574213 (90%)]\tLoss: 0.346965\n",
      "Train Epoch: 10 [2571264/2574213 (100%)]\tLoss: 0.407448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/anaconda3/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2bbfe523cb08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msingle_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/federated-learning-example-for-student/single_node.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(epochs, lr, batch_size, test_batch_size, use_cuda, gamma, save_model)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/federated-learning-example-for-student/single_node.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/anaconda3/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/opt/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "single_node.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
